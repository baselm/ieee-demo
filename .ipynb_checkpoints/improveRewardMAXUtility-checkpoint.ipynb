{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Defiend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "import os\n",
    "from tempfile import mkdtemp\n",
    "from werkzeug import secure_filename\n",
    "import requests\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "from gym import spaces, logger\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "import numpy as np \n",
    "import time \n",
    "import csv\n",
    "import pandas as pd \n",
    "class FooEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    def __init__(self):\n",
    "        print('__init__')\n",
    "        tstart = time.time()\n",
    "        filename1=\"dqn_logs.csv\"\n",
    "        self.fileHandle = open(filename1,\"w\")\n",
    "        self.writer = csv.writer(self.fileHandle)\n",
    "        self.maxNode = 5 \n",
    "        self.minNode =1 \n",
    "        self.node = self.getNumberofNode()\n",
    "        self.cpu_axis  = self.get_cpu_observation()\n",
    "        self.mem_axis = self.get_mem_observation()\n",
    "        self.disk_axis = self.get_disk_observation()\n",
    "        self.net_axis  = self.get_net_observation()\n",
    "        self.action_space = spaces.Discrete(10)\n",
    "        high = np.array([\n",
    "            self.get_cpu_observation(),\n",
    "            self.get_mem_observation(),\n",
    "            self.get_disk_observation(),\n",
    "            self.get_net_observation()])\n",
    "        low = np.array([\n",
    "            np.zeros(5),\n",
    "            np.zeros(5),\n",
    "            np.zeros(5),\n",
    "            np.zeros(5)])\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "        self.seed()\n",
    "        self.obs= 0\n",
    "        self.obs = self.get_observation()\n",
    "        self.viewer = None\n",
    "        self.state = self.get_observation()\n",
    "        self.state_name = 's0'\n",
    "        self.attempt = 0 \n",
    "        self.steps_beyond_done = None\n",
    "        self.done = False\n",
    "        self.adapte_cpu= False  \n",
    "        self.adapte_mem= False \n",
    "        self.adapte_disk= False \n",
    "        self.adapte_net= False \n",
    "        self.writer.writerow([\"timestamp\",\"state\", \"action\", \"reward\", \"maxUtility\", \"duration\", \"info\", \"# node\"])\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "        state = self.get_observation()\n",
    "        #past_stat = self.state\n",
    "        #print(state)\n",
    "        #Find the Utility Prefernces \n",
    "        # Select action \n",
    "        # get the reward value\n",
    "        tstart = time.time()\n",
    "        maxUtility = np.amax(self.obs[:,4])\n",
    "        utilityType = np.argmax(self.obs[:,4])\n",
    "        if utilityType== 0:\n",
    "            self.adapte_cpu=True \n",
    "        elif utilityType== 1:\n",
    "            self.adapte_mem = True\n",
    "        elif utilityType== 2:\n",
    "            self.adapte_disk = True\n",
    "        elif utilityType== 3:\n",
    "            self.adapte_net = True\n",
    "        self.attempt += 1\n",
    "        #print('self.adapte_cpu: ', self.adapte_cpu, 'self.adapte_mem:', self.adapte_mem, 'self.adapte_disk:',self.adapte_disk,'self.adapte_net:', self.adapte_net )\n",
    "        done=False\n",
    "        reward=0\n",
    "        info=''\n",
    "        #Here \n",
    "     \n",
    "        if action == 9 or action==0:\n",
    "            #print(\"Stay in State S0\")\n",
    "            self.obs = self.get_observation()\n",
    "            reward=  np.amax(self.obs[:,3])\n",
    "            self.state_name='s0'\n",
    "        elif action == 1:\n",
    "            #print(\"Stay in State S1\")\n",
    "            self.obs = self.get_observation()\n",
    "            \n",
    "            if (self.attempt>300):\n",
    "                done = True\n",
    "                self.state_name='s1'\n",
    "                reward=  1\n",
    "            else:\n",
    "                done= False \n",
    "            info = \"Stay in State S0\"\n",
    "            self.state_name='s0'\n",
    "            reward=  1- np.amax(self.obs[:,3])  \n",
    "            print(\"reward: \",reward, np.amax(self.obs[:,4]))\n",
    "        elif action == 2: \n",
    "            try:\n",
    "                response = requests.get(' http://192.168.99.100:5000/services/vscale/web/'+ str(self.attempt) + '/' + str(self.cpu_axis[0])+'/'+str(self.cpu_axis[3]))\n",
    "                results = response.json()\n",
    "                if results['result']=='Service converged':\n",
    "                    done=True \n",
    "                    self.obs = self.get_observation()\n",
    "                    reward= 1 - np.amax(self.obs[:,3]) \n",
    "                    self.state_name='s2'\n",
    "                    info = \"Scale Up Move to State S2\"\n",
    "                else:\n",
    "                    done= False\n",
    "                    print(results)\n",
    "                    self.obs = self.get_observation()\n",
    "                    reward= 1- np.amax(self.obs[:,3])  \n",
    "                    print(\"reward: \",reward, np.amax(self.obs[:,4]))\n",
    "                    self.state_name='s0'\n",
    "                    info = \"Scale Up Move to State S0\"\n",
    "            except:\n",
    "                pass\n",
    "            finally:\n",
    "                \n",
    "                pass\n",
    "\n",
    "        elif action == 3: \n",
    "            #print(\"Maintain Cluster State S4 and delete dangling docker containers\")\n",
    "            self.obs = self.get_observation()\n",
    "            reward= 1 - np.amax(self.obs[:,3])\n",
    "            #print(\"reward: \",reward, np.amax(self.obs[:,4]))\n",
    "            done= False\n",
    "            self.state_name='s0'\n",
    "            if (self.attempt>300):\n",
    "                done = True\n",
    "                info = \"delete dangling docker containers S3\"\n",
    "                cur_dir = os.getcwd()\n",
    "                filepath = os.path.join(cur_dir, 'cleancontainers.sh')\n",
    "                print (filepath)\n",
    "                res= subprocess.call(filepath, shell=True)\n",
    "                print (res)\n",
    "                reward = 1\n",
    "                self.state_name='s3'\n",
    "        elif action == 4: \n",
    "            current_state = self.obs\n",
    "            if (self.node <= self.maxNode and self.node >= self.minNode ):\n",
    "                #print(\"Add Node s4\")\n",
    "                cur_dir = os.getcwd()\n",
    "                filepath = os.path.join(cur_dir, 'addNode.sh')\n",
    "                print (filepath)\n",
    "                res= subprocess.call(filepath, shell=True)\n",
    "                print (res)\n",
    "                info = \"Add Node S4\"\n",
    "                self.obs = self.get_observation()\n",
    "                reward= 1 - np.amax(self.obs[:,3])\n",
    "                self.state_name='s4'\n",
    "                print(\"reward: \",reward, np.amax(self.obs[:,4]))\n",
    "                done= True\n",
    "                self.node +=1\n",
    "            elif self.node==5:\n",
    "                    done=True \n",
    "                    reward= 1 \n",
    "                    self.state_name='s4'\n",
    "            else:\n",
    "                #print(\"go back to Cluster State at S0: \", self.attempt)\n",
    "                done= False\n",
    "                self.obs = self.get_observation()\n",
    "                reward= 1 - np.amax(self.obs[:,3])\n",
    "                print(\"reward: \",reward, np.amax(self.obs[:,4]))\n",
    "                self.state_name='s0'\n",
    "        elif action == 5: \n",
    "            self.obs = self.get_observation()\n",
    "            if (self.node <= self.maxNode and self.node > self.minNode ):\n",
    "                #print(\"Delete Node S5\")\n",
    "                self.state_name='s5'\n",
    "                cur_dir = os.getcwd()\n",
    "                filepath = os.path.join(cur_dir, 'deleteNode.sh')\n",
    "                #print (filepath)\n",
    "                res= subprocess.call(filepath, shell=True)\n",
    "                print (res)\n",
    "                info = \"Delete Node S5\"\n",
    "                reward= 1\n",
    "                #print(\"reward: \",reward, np.amax(self.obs[:,4]))\n",
    "                done= True\n",
    "                self.node -=1\n",
    "            else:\n",
    "                #print(\"Maintain Cluster State at S0: \", self.attempt, self.node, self.minNode, self.maxNode)\n",
    "                done= False\n",
    "                reward= 1 - np.amax(self.obs[:,3])\n",
    "                #print(\"reward: \",reward)\n",
    "                self.state_name='s0'\n",
    "                if self.node==1:\n",
    "                    done=True \n",
    "                    reward= 1\n",
    "                    self.state_name='s5'\n",
    "        elif action==6:\n",
    "            #print(\"freedisk Space S6\")\n",
    "            cur_dir = os.getcwd()\n",
    "            filepath = os.path.join(cur_dir, 'freedisk.sh')\n",
    "            #print (filepath)\n",
    "            res= subprocess.call(filepath, shell=True)\n",
    "            #print (res)\n",
    "            info = \"freedisk Node S6\"\n",
    "            self.state_name='s6'\n",
    "            reward= 1\n",
    "            #print(\"reward: \",reward, np.amax(self.obs[:,4]))\n",
    "            done= True\n",
    "        elif action == 7:\n",
    "            #Reward = max of utility fitness\n",
    "            try:\n",
    "                print(\"rollback and enforce new cluster\")\n",
    "                cur_dir = os.getcwd()\n",
    "                filepath = os.path.join(cur_dir, 'newcluster.sh')\n",
    "                print (filepath)\n",
    "                res= subprocess.call(filepath, shell=True)\n",
    "                print (res)\n",
    "                self.state_name='s7'\n",
    "                info = \"rollback and enforce new  cluster S7\"\n",
    "                time.sleep(300)\n",
    "                reward= 1\n",
    "                #print(\"reward: \",reward, np.amax(self.obs[:,4]))\n",
    "                done= True\n",
    "                #print(reward)\n",
    "                info = \"rollback and enforce new cluster\"\n",
    "            except:\n",
    "                pass\n",
    "            finally:\n",
    "                pass\n",
    "        elif action==8:\n",
    "            if (self.node < self.maxNode-2 and self.node >= self.minNode ):\n",
    "                self.state_name='s8'\n",
    "                cur_dir = os.getcwd()\n",
    "                filepath = os.path.join(cur_dir, 'addNode.sh')\n",
    "                #print (filepath)\n",
    "                res= subprocess.call(filepath, shell=True)\n",
    "                #print (res)\n",
    "                info = \"Add Manager node S8\"\n",
    "                self.node += 1\n",
    "                reward= 1\n",
    "                #print(\"reward: \",reward, np.amax(self.obs[:,4]), \"Node: \", self.node)\n",
    "                done= True\n",
    "            elif (self.node >= self.maxNode):\n",
    "                info = \"Maintain Manager nodes S0\"\n",
    "                self.state_name='s8'\n",
    "                reward= 1 - np.amax(self.obs[:,3])\n",
    "                #print(\"reward: \",reward, np.amax(self.obs[:,4]), \"Node: \", self.node)\n",
    "                done= True \n",
    "            else:\n",
    "                #print(\"Maintain Cluster State S0\", self.attempt, self.node )\n",
    "                self.state_name='s0'\n",
    "                reward= 1 - np.amax(self.obs[:,3])\n",
    "                done= False\n",
    "                #print(\"reward: \",reward, np.amax(self.obs[:,4]))\n",
    "        else: \n",
    "            print (\"action not defined\")\n",
    "            self.state_name='s0'\n",
    "            self.obs = self.get_observation()\n",
    "            done= False\n",
    "            reward= -1  \n",
    "            info = \"action not defined\"\n",
    "        if done: \n",
    "            reward = 1\n",
    "        elif self.steps_beyond_done is None:\n",
    "            #Adaptation Failed \n",
    "            reward = 0.0 \n",
    "            self.steps_beyond_done = 0\n",
    "        else: \n",
    "            if self.steps_beyond_done == 1:\n",
    "                logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
    "                self.steps_beyond_done += 1\n",
    "                reward = 0.0\n",
    "        tend = tstart - time.time()\n",
    "        print (\"State: \", self.state_name, \"action: \", action, \"reward: \", reward )\n",
    "        #writer1.writerow([\"timestamp\",\"state\", \"action\", \"reward\", \"maxUtility\", \"duration\"])\n",
    "        self.writer.writerow([tstart,self.state_name, action, reward, np.amax(self.obs[:,4]), tend, info, self.node])\n",
    "        return self.obs, reward, done, {}\n",
    "    def reset(self):\n",
    "        #self.fileHandle.close()\n",
    "        self.state = self.get_observation()\n",
    "        self.steps_beyond_done = None\n",
    "        self.adapte_cpu= False  \n",
    "        self.adapte_mem= False \n",
    "        self.adapte_disk= False \n",
    "        self.adapte_net= False \n",
    "        self.maxNode = 5 \n",
    "        self.minNode =1 \n",
    "        return np.array(self.state)\n",
    "    def render(self, mode='human', close=False):\n",
    "        logger.warn(\"View is not allowed in this environment\")\n",
    "        return 0 \n",
    "    def close(self):\n",
    "        self.fileHandle.close()\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "    def get_observation(self):\n",
    "        try:\n",
    "            self.disk_axis = self.get_disk_observation() \n",
    "            self.mem_axis = self.get_mem_observation()\n",
    "            self.cpu_axis = self.get_cpu_observation()\n",
    "            self.net_axis = self.get_net_observation()\n",
    "            obs =np.vstack((self.cpu_axis,self.mem_axis, self.disk_axis, self.net_axis) )\n",
    "            return obs\n",
    "        except:\n",
    "            pass\n",
    "        finally: \n",
    "            pass \n",
    "    def get_cpu_observation(self):\n",
    "        try: \n",
    "            response = requests.get('http://192.168.99.100:8888/cpu', timeout=5)\n",
    "            results = response.json()\n",
    "            if len(results) > 0:\n",
    "                    cpu = results['cpu']\n",
    "                    prediction = results['prediction']\n",
    "                    anomalyScore = results['anomalyScore']\n",
    "                    anomalyLikelihood = results['anomalyLikelihood']\n",
    "                    utility_cpu = results['utility_cpu']\n",
    "                    cpu_axis=[cpu, prediction, anomalyScore, anomalyLikelihood, utility_cpu]\n",
    "        except:\n",
    "            cpu_axis=[-1, -1, -1, -1, -1]\n",
    "            pass    \n",
    "        finally: \n",
    "            pass\n",
    "        return np.array(cpu_axis)\n",
    "    def get_mem_observation(self):\n",
    "        try:\n",
    "            response = requests.get('http://192.168.99.100:8888/mem', timeout=5)\n",
    "            results = response.json()\n",
    "            if len(results) > 0:\n",
    "                mem = results['mem']\n",
    "                prediction = results['prediction']\n",
    "                anomalyScore = results['anomalyScore']\n",
    "                anomalyLikelihood = results['anomalyLikelihood']\n",
    "                utility_mem = results['utility_mem']\n",
    "                mem_axis=[mem, prediction, anomalyScore, anomalyLikelihood, utility_mem]\n",
    "        except:\n",
    "            mem_axis=[-1, -1, -1, -1, -1]\n",
    "            pass     \n",
    "        finally: \n",
    "            #mem_axis=[0, 0, 0, 0, 0]\n",
    "            pass\n",
    "        return np.array(mem_axis)\n",
    "    def get_net_observation(self):\n",
    "        try:\n",
    "            response = requests.get('http://192.168.99.100:8888/net', timeout=5)\n",
    "            results = response.json()\n",
    "            if len(results) > 0:\n",
    "                net = results['net']\n",
    "                prediction = results['prediction']\n",
    "                anomalyScore = results['anomalyScore']\n",
    "                anomalyLikelihood = results['anomalyLikelihood']\n",
    "                utility_net = results['utility_net']\n",
    "                net_axis=[net, prediction, anomalyScore, anomalyLikelihood, utility_net]\n",
    "        except:\n",
    "            net_axis=[-1, -1, -1, -1, -1]\n",
    "            pass\n",
    "        finally:\n",
    "            pass \n",
    "        return np.array(net_axis)\n",
    "    def get_disk_observation(self):\n",
    "        try:\n",
    "            response = requests.get('http://192.168.99.100:8888/disk', timeout=5)\n",
    "            if response is not None:\n",
    "                results = response.json()\n",
    "                if len(results) > 0:\n",
    "                    disk = results['disk']\n",
    "                    prediction = results['prediction']\n",
    "                    anomalyScore = results['anomalyScore']\n",
    "                    anomalyLikelihood = results['anomalyLikelihood']\n",
    "                    utility_disk = results['utility_disk']\n",
    "            disk_axis=[disk, prediction, anomalyScore, anomalyLikelihood, utility_disk]\n",
    "        except:\n",
    "            disk_axis=[-1, -1, -1, -1, -1]\n",
    "            pass\n",
    "        finally:\n",
    "            \n",
    "            pass\n",
    "        return np.array(disk_axis)\n",
    "\n",
    "    def get_current_state(self):\n",
    "        current_state = self.state\n",
    "         \n",
    "        return current_state\n",
    "    def closeFile(self):\n",
    "        self.fileHandle.close()\n",
    "    def getNumberofNode(self):\n",
    "        try:\n",
    "            prometheus='192.168.99.100'\n",
    "            tstart = time.time()\n",
    "            end = str(tstart+360)\n",
    "            start = str(tstart)\n",
    "            node = 1\n",
    "            q = 'count(node_meta%20*%20on(instance)%20group_left(node_name)%20node_meta%7Bnode_id%3D~\".%2B\"%7D)'\n",
    "            q1 = 'count(node_meta * on(instance) group_left(node_name) node_meta{node_id=~\"$node_id\"})'\n",
    "            response = requests.get('http://admin:admin@'+prometheus+':9090/api/v1/query_range?query='+q+'&start='+start+'&end='+ end + '&step=120', timeout=10)\n",
    "            if response is not None:\n",
    "                results = response.json()\n",
    "                Data = results['data']['result']\n",
    "                print(len(Data))\n",
    "                if len(Data)> 0:\n",
    "                    Value = Data[0]['values']\n",
    "                    node = Value[0][1]\n",
    "            else:\n",
    "                node=1\n",
    "            return int(node)\n",
    "        except:\n",
    "            node=1\n",
    "        finally:\n",
    "           \n",
    "            pass\n",
    "        return node\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### call the agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_foo\n",
    "\n",
    "env = FooEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28.41431714, 28.41431714,  1.        ,  0.5       ,  0.        ],\n",
       "       [40.18634906, 40.18634906,  1.        ,  0.5       ,  0.        ],\n",
       "       [98.        , 98.        ,  1.        ,  0.5       ,  0.        ],\n",
       "       [57.33333333, 57.33333333,  1.        ,  0.5       ,  0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  call Keras API and Keras rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import csv\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import subprocess\n",
    "from subprocess import call\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory \n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # # MDP \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Deep Q-Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_actions= env.action_space.n\n",
    "nb_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.closeFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                336       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                170       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,050\n",
      "Trainable params: 1,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=4200, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=200,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 3200 steps ...\n",
      "/Users/baz/ieee-demo/addNode.sh\n",
      "0\n",
      "reward:  0.5 0.0\n",
      "State:  s4 action:  4 reward:  1\n",
      "    1/3200: episode: 1, duration: 165.666s, episode steps: 1, steps per second: 0, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 4.000 [4.000, 4.000], mean observation: 19.636 [0.000, 98.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "State:  s0 action:  9 reward:  0.0\n",
      "0\n",
      "State:  s5 action:  5 reward:  1\n",
      "    3/3200: episode: 2, duration: 21.507s, episode steps: 2, steps per second: 0, episode reward: 1.000, mean reward: 0.500 [0.000, 1.000], mean action: 7.000 [5.000, 9.000], mean observation: 405.143 [0.000, 7650.487], loss: --, mean_absolute_error: --, mean_q: --\n",
      "State:  s0 action:  9 reward:  0.0\n",
      "/Users/baz/ieee-demo/addNode.sh\n",
      "0\n",
      "reward:  0.5 0.0\n",
      "State:  s4 action:  4 reward:  1\n",
      "    5/3200: episode: 3, duration: 179.519s, episode steps: 2, steps per second: 0, episode reward: 1.000, mean reward: 0.500 [0.000, 1.000], mean action: 6.500 [4.000, 9.000], mean observation: 51.096 [0.000, 630.958], loss: --, mean_absolute_error: --, mean_q: --\n",
      "State:  s0 action:  9 reward:  0.0\n",
      "/Users/baz/ieee-demo/addNode.sh\n",
      "0\n",
      "reward:  0.5 0.0\n",
      "State:  s4 action:  4 reward:  1\n",
      "    7/3200: episode: 4, duration: 167.724s, episode steps: 2, steps per second: 0, episode reward: 1.000, mean reward: 0.500 [0.000, 1.000], mean action: 6.500 [4.000, 9.000], mean observation: 220.964 [0.000, 3364.376], loss: --, mean_absolute_error: --, mean_q: --\n",
      "/Users/baz/ieee-demo/addNode.sh\n",
      "0\n",
      "reward:  0.5 0.0\n",
      "State:  s4 action:  4 reward:  1\n",
      "    8/3200: episode: 5, duration: 145.026s, episode steps: 1, steps per second: 0, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 4.000 [4.000, 4.000], mean observation: 399.564 [0.000, 3834.933], loss: --, mean_absolute_error: --, mean_q: --\n",
      "State:  s0 action:  9 reward:  0.0\n",
      "/Users/baz/ieee-demo/addNode.sh\n",
      "0\n",
      "reward:  0.5 0.0\n",
      "State:  s4 action:  4 reward:  1\n",
      "   10/3200: episode: 6, duration: 159.158s, episode steps: 2, steps per second: 0, episode reward: 1.000, mean reward: 0.500 [0.000, 1.000], mean action: 6.500 [4.000, 9.000], mean observation: 89.650 [0.000, 1343.290], loss: --, mean_absolute_error: --, mean_q: --\n",
      "0\n",
      "State:  s5 action:  5 reward:  1\n",
      "   11/3200: episode: 7, duration: 14.252s, episode steps: 1, steps per second: 0, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 5.000 [5.000, 5.000], mean observation: 14.374 [0.000, 56.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "State:  s0 action:  9 reward:  0.0\n",
      "/Users/baz/ieee-demo/addNode.sh\n",
      "0\n",
      "reward:  0.5 0.0\n",
      "State:  s4 action:  4 reward:  1\n",
      "   13/3200: episode: 8, duration: 159.031s, episode steps: 2, steps per second: 0, episode reward: 1.000, mean reward: 0.500 [0.000, 1.000], mean action: 6.500 [4.000, 9.000], mean observation: 21.293 [0.000, 140.295], loss: --, mean_absolute_error: --, mean_q: --\n",
      "/Users/baz/ieee-demo/addNode.sh\n",
      "0\n",
      "reward:  0.5 0.0\n",
      "State:  s4 action:  4 reward:  1\n",
      "   14/3200: episode: 9, duration: 193.257s, episode steps: 1, steps per second: 0, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 4.000 [4.000, 4.000], mean observation: 28.819 [0.000, 173.072], loss: --, mean_absolute_error: --, mean_q: --\n",
      "reward:  0.5 0.0\n",
      "State:  s0 action:  4 reward:  0.0\n",
      "State:  s0 action:  5 reward:  0.5\n",
      "reward:  0.5 0.0\n",
      "State:  s0 action:  4 reward:  0.5\n",
      "State:  s0 action:  5 reward:  0.5\n",
      "State:  s6 action:  6 reward:  1\n",
      "   19/3200: episode: 10, duration: 6408.426s, episode steps: 5, steps per second: 0, episode reward: 2.500, mean reward: 0.500 [0.000, 1.000], mean action: 4.800 [4.000, 6.000], mean observation: 16.150 [0.000, 236.944], loss: --, mean_absolute_error: --, mean_q: --\n",
      "State:  s6 action:  6 reward:  1\n",
      "   20/3200: episode: 11, duration: 8.413s, episode steps: 1, steps per second: 0, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 6.000 [6.000, 6.000], mean observation: 11.272 [0.000, 56.304], loss: --, mean_absolute_error: --, mean_q: --\n",
      "State:  s0 action:  5 reward:  0.0\n",
      "State:  s8 action:  8 reward:  1\n",
      "   22/3200: episode: 12, duration: 0.248s, episode steps: 2, steps per second: 8, episode reward: 1.000, mean reward: 0.500 [0.000, 1.000], mean action: 6.500 [5.000, 8.000], mean observation: -1.000 [-1.000, -1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "State:  s0 action:  5 reward:  0.0\n",
      "reward:  2 -1\n",
      "State:  s0 action:  1 reward:  2\n",
      "State:  s0 action:  3 reward:  2\n",
      "State:  s0 action:  5 reward:  2\n",
      "rollback and enforce new cluster\n",
      "/Users/baz/ieee-demo/newcluster.sh\n",
      "0\n",
      "State:  s7 action:  7 reward:  1\n",
      "   27/3200: episode: 13, duration: 433.501s, episode steps: 5, steps per second: 0, episode reward: 7.000, mean reward: 1.400 [0.000, 2.000], mean action: 4.200 [1.000, 7.000], mean observation: -1.000 [-1.000, -1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "State:  s0 action:  9 reward:  0.0\n",
      "State:  s0 action:  5 reward:  2\n",
      "State:  s0 action:  9 reward:  -1\n",
      "State:  s0 action:  5 reward:  2\n",
      "State:  s0 action:  5 reward:  2\n",
      "State:  s8 action:  8 reward:  1\n",
      "   33/3200: episode: 14, duration: 0.236s, episode steps: 6, steps per second: 25, episode reward: 6.000, mean reward: 1.000 [-1.000, 2.000], mean action: 6.833 [5.000, 9.000], mean observation: -1.000 [-1.000, -1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "State:  s0 action:  5 reward:  0.0\n",
      "reward:  2 -1\n",
      "State:  s0 action:  4 reward:  2\n",
      "State:  s0 action:  5 reward:  2\n",
      "State:  s0 action:  9 reward:  -1\n",
      "reward:  2 -1\n",
      "State:  s0 action:  4 reward:  2\n",
      "State:  s0 action:  0 reward:  -1\n",
      "State:  s0 action:  2 reward:  0\n",
      "State:  s0 action:  5 reward:  2\n",
      "State:  s0 action:  9 reward:  -1\n",
      "State:  s0 action:  0 reward:  -1\n",
      "State:  s0 action:  5 reward:  2\n",
      "State:  s6 action:  6 reward:  1\n",
      "   45/3200: episode: 15, duration: 1.194s, episode steps: 12, steps per second: 10, episode reward: 7.000, mean reward: 0.583 [-1.000, 2.000], mean action: 4.500 [0.000, 9.000], mean observation: -1.000 [-1.000, -1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "State:  s0 action:  9 reward:  0.0\n",
      "rollback and enforce new cluster\n",
      "/Users/baz/ieee-demo/newcluster.sh\n",
      "0\n",
      "State:  s7 action:  7 reward:  1\n",
      "   47/3200: episode: 16, duration: 339.901s, episode steps: 2, steps per second: 0, episode reward: 1.000, mean reward: 0.500 [0.000, 1.000], mean action: 8.000 [7.000, 9.000], mean observation: -1.000 [-1.000, -1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "State:  s0 action:  5 reward:  0.0\n",
      "State:  s8 action:  8 reward:  1\n",
      "   49/3200: episode: 17, duration: 34.499s, episode steps: 2, steps per second: 0, episode reward: 1.000, mean reward: 0.500 [0.000, 1.000], mean action: 6.500 [5.000, 8.000], mean observation: -1.000 [-1.000, -1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "State:  s0 action:  5 reward:  0.0\n",
      "State:  s0 action:  9 reward:  -1\n",
      "State:  s0 action:  9 reward:  -1\n",
      "State:  s0 action:  5 reward:  2\n",
      "State:  s0 action:  5 reward:  2\n",
      "State:  s0 action:  5 reward:  2\n",
      "State:  s0 action:  3 reward:  2\n",
      "State:  s0 action:  5 reward:  2\n",
      "rollback and enforce new cluster\n",
      "/Users/baz/ieee-demo/newcluster.sh\n",
      "State:  s0 action:  7 reward:  0\n",
      "State:  s0 action:  5 reward:  2\n",
      "State:  s0 action:  9 reward:  -1\n",
      "State:  s0 action:  0 reward:  -1\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ENV_NAME=\"improvedRewardv12\"\n",
    "weights_filename = 'mydqn_{}_weights.h5f'.format(ENV_NAME)\n",
    "checkpoint_weights_filename = 'dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
    " \n",
    "log_filename = 'mydqn_{}_log.json'.format(ENV_NAME)\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=3200)]\n",
    "callbacks += [FileLogger(log_filename, interval=3200)]\n",
    "callbacks += [TensorBoard(log_dir='./dqn_logsv3/', histogram_freq=0, write_graph=False)]\n",
    "dqn.fit(env, callbacks=callbacks, nb_steps=3200, log_interval=3200,verbose=2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights(weights_filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.closeFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights(weights_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=10,  visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "ENV_NAME=\"micro4\"\n",
    "weights_filename = 'mydqn_{}_weights.h5f'.format(ENV_NAME)\n",
    "checkpoint_weights_filename = 'dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
    "log_filename = 'mydqn_{}_log.json'.format(ENV_NAME)\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
    "callbacks += [FileLogger(log_filename, interval=100)]\n",
    "dqn.fit(env, callbacks=callbacks, nb_steps=175, log_interval=10000)\n",
    "\n",
    "#dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_log(filename, figsize=None, output=None):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    if 'episode' not in data:\n",
    "        raise ValueError('Log file \"{}\" does not contain the \"episode\" key.'.format(filename))\n",
    "    episodes = data['episode']\n",
    "\n",
    "    \n",
    "\n",
    "    # Get value keys. The x axis is shared and is the number of episodes.\n",
    "    keys = sorted(list(set(data.keys()).difference(set(['episode']))))\n",
    "\n",
    "    if figsize is None:\n",
    "        figsize = (35., 15. * len(keys))\n",
    "    f, axarr = plt.subplots(len(keys), sharex=True, figsize=figsize)\n",
    "    for idx, key in enumerate(keys):\n",
    "        axarr[idx].plot(episodes, data[key])\n",
    "        axarr[idx].set_ylabel(key)\n",
    "        \n",
    "    plt.xlabel('episodes')\n",
    "    plt.tight_layout()\n",
    "    if output is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_log(args.filename, output=args.output, figsize=args.figsize)\n",
    "visualize_log('mydqn_improvedRewardv10_log.json')\n",
    "#mydqn_micro_with_swarmed_log.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('mydqn_micro_with_swarmed_log.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=[9.4, 8.8], dpi=150)\n",
    "ax1 = fig.add_subplot(211)\n",
    "t = data['nb_steps']\n",
    "s1 = data['duration']\n",
    "ax1.plot(t, s1, 'b-', markevery=(300,8))\n",
    "ax1.set_xlabel('Number of Episodes')\n",
    "ax1.set_ylabel('Reward per Episode')\n",
    "ax1.grid(True)\n",
    "\n",
    " \n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data['duration']\n",
    " \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t)\n",
    "\n",
    "ax.set(xlabel='time (s)', ylabel='voltage (mV)',\n",
    "       title='About as simple as it gets, folks')\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(data['duration'], bins=50,  kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#data = [1.5]*7 + [2.5]*2 + [3.5]*8 + [4.5]*3 + [5.5]*1 + [6.5]*8\n",
    "sns.set_style('whitegrid')\n",
    "sns.kdeplot(data['duration'], bw=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t =  data['duration']\n",
    "s = data['nb_steps']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(s, t)\n",
    "\n",
    "ax.grid(True, linestyle='-.')\n",
    "ax.tick_params(labelcolor='b', labelsize='medium', width=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "fig = plt.figure(figsize=[9.4, 8.8], dpi=150)\n",
    "ax1 = fig.add_subplot(111)\n",
    "t = data['episode']\n",
    "s1 = data['episode_reward']\n",
    "ax1.plot(t, s1, 'b-', markevery=(30,8))\n",
    "ax1.set_xlabel('Number of Episodes')\n",
    "ax1.set_ylabel('Reward per Episode')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "s2 = data['mean_q']\n",
    "ax2.plot(t, s2, 'go--', markevery=0.1)\n",
    "ax2.set_ylabel('Mean of Q value')\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "fig = plt.figure(figsize=[9.4, 8.8], dpi=150)\n",
    "ax1 = fig.add_subplot(111)\n",
    "t = data['episode']\n",
    "s1 = data['duration']\n",
    "ax1.plot(t, s1, 'b-', markevery=(30,8))\n",
    "ax1.set_xlabel('Number of Episodes')\n",
    "ax1.set_ylabel('Adaptation time (seconds)')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "s2 = data['loss']\n",
    "ax2.plot(t, s2, 'r--', markevery=8)\n",
    "ax2.set_ylabel('Mean of Q value')\n",
    "#ax2.set_yscale()\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.hist(bins=50, figsize=(20,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data['mean_q'],'bo')\n",
    "#ax.plot(np.random.rand(20), '-o', ms=20, lw=2, alpha=0.7, mfc='orange')\n",
    "ax.grid()\n",
    "ax.set_xlabel('Mean Q value')\n",
    "#fig.figimage(10, 10, zorder=3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['duration'].plot(kind='bar', grid=True, sharex=True, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "rs = np.random.RandomState(10)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, axes = plt.subplots(nrows=1, ncols=1)\n",
    "#sns.despine(left=True)\n",
    "\n",
    "# Generate a random univariate dataset\n",
    "d = data['duration']\n",
    "d2= data['mean_absolute_error']\n",
    "# Plot a simple histogram with binsize determined automatically\n",
    "#sns.distplot(d, kde=False, color=\"b\", ax=axes[0, 0])\n",
    "\n",
    "# Plot a kernel density estimate and rug plot\n",
    "#sns.distplot(d, kde=True, hist=True, rug=False, color=\"r\", ax=axes[0])\n",
    "\n",
    "# Plot a filled kernel density estimate\n",
    "sns.distplot(d2, kde=True, hist=True, color=\"g\", kde_kws={\"shade\": False}, axlabel='Mean Absolute Error (MAE) ')\n",
    "\n",
    "# Plot a historgram and kernel density estimate\n",
    "#sns.distplot(d2, color=\"m\", ax=axes[1, 1])\n",
    "\n",
    "plt.setp(axes, yticks=[])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data['duration'].plot(kind=\"line\",  legend=True, grid=True, figsize=(40,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True, expand_nested=True, rankdir='TB', to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
